{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMEw-2YDW1yH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import google.generativeai as genai\n",
        "from collections import Counter\n",
        "import re\n",
        "import csv\n",
        "import time\n",
        "import ast\n",
        "import json5\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFGNngcOW1oQ"
      },
      "outputs": [],
      "source": [
        "#here, having only 1 gemini API would do, but to improve performance, we can use multiple APIs\n",
        "gemini_api = 'your_gemini_api'\n",
        "gemini_api_2 = 'optional_gemini_api'\n",
        "gemini_api_3 = 'optional_gemini_api'\n",
        "gemini_api_4 = 'optional_gemini_api'\n",
        "gemini_api_5 = 'optional_gemini_api'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtMASj92Eili"
      },
      "outputs": [],
      "source": [
        "def escape_inner_double_quotes(s):\n",
        "    def replacer(match):\n",
        "        content = match.group(1)\n",
        "        fixed_content = re.sub(r'(?<!\\\\)\"', r'\\\\\"', content)\n",
        "        return f'\"{fixed_content}\"'\n",
        "    pattern = r'\"((?:[^\"\\\\]|\\\\.)*)\"'\n",
        "    return re.sub(pattern, replacer, s)\n",
        "\n",
        "def robust_json_loads(raw_output):\n",
        "    cleaned = raw_output.strip()\n",
        "    if cleaned.startswith(\"```\") and cleaned.endswith(\"```\"):\n",
        "        cleaned = cleaned[3:-3].strip()\n",
        "    lines = cleaned.splitlines()\n",
        "    if lines and lines[0].strip().lower() == \"json\":\n",
        "        cleaned = \"\\n\".join(lines[1:]).strip()\n",
        "    cleaned = cleaned.strip(\"`\").strip()\n",
        "    cleaned = re.sub(r'[\\x00-\\x1f]+', ' ', cleaned)\n",
        "    m = re.search(r'\\[.*\\]', cleaned, re.DOTALL)\n",
        "    candidate = m.group(0) if m else cleaned\n",
        "    candidate_fixed = re.sub(r'\"\\s+\"', '\", \"', candidate)\n",
        "    candidate_fixed2 = escape_inner_double_quotes(candidate_fixed)\n",
        "    parsers = [\n",
        "        (\"json.loads\", json.loads),\n",
        "        (\"ast.literal_eval\", ast.literal_eval),\n",
        "        (\"unicode_escape\", lambda s: json.loads(s.encode('utf-8').decode('unicode_escape'))),\n",
        "        (\"json5.loads\", json5.loads)\n",
        "    ]\n",
        "    for candidate_to_try in [candidate, candidate_fixed, candidate_fixed2]:\n",
        "        for name, parse_func in parsers:\n",
        "            try:\n",
        "                result = parse_func(candidate_to_try)\n",
        "                if isinstance(result, list):\n",
        "                    return result\n",
        "                else:\n",
        "                    print(f\"{name} did not return a list.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error using {name} on candidate: {e}\")\n",
        "    print(\"All parsing attempts failed. Final candidate output:\")\n",
        "    print(candidate)\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iF0sRTifW1UI"
      },
      "outputs": [],
      "source": [
        "toggle = 0\n",
        "async def augment_comments(comments):\n",
        "    if not comments or not isinstance(comments, list):\n",
        "        print(\"Invalid or empty comments list provided.\")\n",
        "        return None\n",
        "\n",
        "    prompt = f'''\n",
        "    You are a comment augmentation model. Your task is to take a list of clean YouTube comments and generate augmented versions of each comment. The augmented comment should:\n",
        "\n",
        "    DO:\n",
        "    - Retain the original sentiment and meaning exactly.\n",
        "    - Introduce linguistic diversity by varying vocabulary, sentence structure, and phrasing.\n",
        "    - Preserve all domain-specific terminology and details so that the context remains intact.\n",
        "    - Produce an output that is distinctly different from the input while conveying the same sentiment.\n",
        "    - Ensure the augmented output is valid and properly formatted as a JSON array.\n",
        "\n",
        "    DON'T:\n",
        "    - Alter the sentiment (a neutral comment must remain neutral).\n",
        "    - Introduce any unrelated or extraneous information beyond rephrasing.\n",
        "    - Use overly complex or unnatural language that doesn't match typical conversational style.\n",
        "\n",
        "    Additional Instructions:\n",
        "    - If the input comment is in a non-English language, translate it into English while preserving its original sentiment and tone.\n",
        "    - **IMPORTANT:** When producing the output, ensure that each augmented comment is enclosed in double quotes (\" \") and that every internal double quotes are correctly escaped (for example, use \\\" for an internal quote).\n",
        "    - If a comment contains a double quoted word like \"word\", then its quotations must be escaped like this \\\"word\\\" in the output.\n",
        "    - Do not mix quote types (avoid starting with a single quote and ending with a double quote, or vice versa).\n",
        "\n",
        "    Input format (comments):\n",
        "    {comments}\n",
        "\n",
        "    Response format should be exactly like the following (horizontal format):\n",
        "    [\"Augmented Comment 1\", \"Augmented Comment 2\", \"Augmented Comment 3\", ...]\n",
        "\n",
        "    Not like the following (vertical format):\n",
        "    [\"Augmented Comment 1\",\n",
        "    \"Augmented Comment 2\",\n",
        "    \"Augmented Comment 3\",\n",
        "    ...]\n",
        "\n",
        "    Do not include any extra explanations or commentary; simply return the list of augmented comments in the exact order as provided in the input.\n",
        "'''\n",
        "    #This is the logic to toggle each APIs, since gemini has rate limit of 10 request per minute, you can modify this logic as per your APIs usage.\n",
        "    global toggle\n",
        "    api_keys = [gemini_api, gemini_api_2, gemini_api_3, gemini_api_4, gemini_api_5]\n",
        "    api_key = api_keys[toggle % 5]\n",
        "    toggle += 1\n",
        "\n",
        "    genai.configure(api_key=api_key)\n",
        "    model = genai.GenerativeModel(model_name=\"gemini-2.0-flash-exp\")\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        if response and response.text:\n",
        "            raw_output = response.text.strip()\n",
        "            try:\n",
        "                aug_comment_match = re.search(r'\\[.*\\]', raw_output, re.DOTALL)\n",
        "                if aug_comment_match:\n",
        "                    aug_comments = robust_json_loads(aug_comment_match.group(0))\n",
        "                else:\n",
        "                    print(\"Raw output:\", raw_output)\n",
        "                    print(\"No augmented comments found in the response.\")\n",
        "                    return None\n",
        "                if aug_comments is None:\n",
        "                    print(\"robust_json_loads returned None.\")\n",
        "                    return None\n",
        "                if len(aug_comments) == len(comments):\n",
        "                    print(f'len of comments: {len(comments)}, len of augmented comments: {len(aug_comments)}')\n",
        "                    return aug_comments\n",
        "                else:\n",
        "                    print(f\"Mismatch between number of comments and augmented comments. len of comments: {len(comments)}, len of augmented comments: {len(aug_comments)}\\nRaw output: {raw_output}\\nAugmented comments: {aug_comments}\")\n",
        "                    return None\n",
        "            except Exception as e:\n",
        "                print(f\"Error during robust JSON parsing: {e}\\nRaw output: {raw_output}\")\n",
        "                return None\n",
        "        else:\n",
        "            print(\"Empty or invalid response from Gemini.\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during comment augmentation: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comments = [\n",
        "    \"Oh great, the 'strategy' is back. Let's see how well that works out this time.\",\n",
        "    \"Here comes the 'professional' again, ready to turn his chips into dust.\",\n",
        "    \"'Expert' player alert—don’t hold your breath for a win though.\",\n",
        "    \"Well, well, the 'high roller' is at it again, about to blow his last stack.\",\n",
        "    \"Look out, everyone, the 'wizard' of blackjack is in the building… prepare for disaster.\",\n",
        "    \"The 'mastermind' strikes once more—this should be fun to watch (for all the wrong reasons).\",\n",
        "    \"Oh, the 'champion' is back, let's hope his luck improves this time (it won’t).\",\n",
        "    \"A 'winning' streak? Nah, that’s not really his thing… unless you count losses.\",\n",
        "    \"Another day, another round of 'perfect' decisions by the so-called 'pro'.\",\n",
        "    \"'Lucky' gambler strikes again—too bad the only thing he's winning is debt.\",\n",
        "    \"Isn’t it cute how the 'guru' always finds new ways to lose big? Classic.\"\n",
        "]"
      ],
      "metadata": {
        "id": "PTPLaK89J0eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "await augment_comments(comments)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "6I6r7rF8Juou",
        "outputId": "59820170-1e31-4488-f2d8-085105cae7d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len of comments: 11, len of augmented comments: 11\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ah, the \"strategy\" returns. I\\'m eager to observe its effectiveness this time around.',\n",
              " 'Here comes the \"professional\" again, all set to convert his chips into thin air, I see.',\n",
              " '\"Expert\" player in the house – though I wouldn\\'t expect a victory anytime soon.',\n",
              " 'Well, look who it is, the \"high roller\" back again, probably ready to gamble away his final chips.',\n",
              " 'Watch out, folks, the blackjack \"wizard\" has arrived… brace yourselves for a train wreck.',\n",
              " 'The \"mastermind\" makes another appearance—this promises to be entertaining, albeit for all the wrong reasons.',\n",
              " 'Oh look, the \"champion\" is back; let\\'s hope his fortune changes this time (spoiler: it won’t).',\n",
              " 'A \"winning\" streak? Not really his forte… unless you are counting all the losses.',\n",
              " 'Another day, another set of \"perfect\" choices from our esteemed \\'pro\\'.',\n",
              " '\"Lucky\" gambler is back at it—too bad he\\'s really just racking up more debt.',\n",
              " 'Isn’t it adorable how the \"guru\" consistently discovers fresh methods for substantial losses? Truly a classic.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUhUPj8-W1Jz"
      },
      "outputs": [],
      "source": [
        "def clean(comment):\n",
        "    return re.sub(r'\\d{1,2}(:\\d{2}){1,3}|\\s+', ' ', comment).strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WP9NrNCS35L"
      },
      "outputs": [],
      "source": [
        "def published_at():\n",
        "    return datetime.datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vxRC1ef3bfW"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('path_to_comments')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAIuPjJf585C",
        "outputId": "36f56f06-cb56-47fc-b9df-23d3cc04b70b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55000"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlkxH-QWW1AQ"
      },
      "outputs": [],
      "source": [
        "async def augmentation_comment_batch(df):\n",
        "  df_negative = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "  print(f\"Total negative comments: {len(df_negative)}\")\n",
        "  augmented_rows = []\n",
        "  batch_size = 20\n",
        "  request_count = 0\n",
        "  start_time = time.time()\n",
        "  tot_com = 0\n",
        "  failed_com = 0\n",
        "  for i in range(0, len(df_negative), batch_size):\n",
        "    batch_df = df_negative.iloc[i: i + batch_size]\n",
        "    original_comments = [clean(com) for com in batch_df[\"Comment\"].tolist()]\n",
        "    tot_com += len(original_comments)\n",
        "    if request_count == 45:\n",
        "      elapsed_time = time.time() - start_time\n",
        "      sleep_time = max(0, 90 - elapsed_time)\n",
        "      sleep_time = sleep_time if sleep_time >= 10 else 10\n",
        "      print(f\"Rate limit reached. Sleeping for {sleep_time} seconds.\")\n",
        "      time.sleep(sleep_time)\n",
        "      request_count = 0\n",
        "      start_time = time.time()\n",
        "    try:\n",
        "      augmented_comments = await augment_comments(original_comments)\n",
        "      request_count += 1\n",
        "    except Exception as e:\n",
        "      print(f\"Error during augmentation: {e}\")\n",
        "      failed_com += len(original_comments)\n",
        "      continue\n",
        "    if augmented_comments is None:\n",
        "      failed_com += len(original_comments)\n",
        "      continue\n",
        "    for i, (_, row) in enumerate(batch_df.iterrows()):\n",
        "      new_row = {\n",
        "          \"ComId\": str(row[\"ComId\"]) + \"_aug\",\n",
        "          \"Vid\": row[\"Vid\"],\n",
        "          \"VideoTitle\": row[\"VideoTitle\"],\n",
        "          \"AuthorName\": \"AugmentedUser\",\n",
        "          \"AuthorCid\": \"AugmentedCID\",\n",
        "          \"Comment\": augmented_comments[i],\n",
        "          \"Sentiment\": row[\"Sentiment\"],\n",
        "          \"LikeCount\": 0,\n",
        "          \"ReplyCount\": 0,\n",
        "          \"PublishedAt\": published_at(),\n",
        "          \"RegionCode\": row[\"RegionCode\"],\n",
        "          \"CategoryId\": row[\"CategoryId\"]\n",
        "      }\n",
        "      augmented_rows.append(new_row)\n",
        "    if tot_com % 200 == 0:\n",
        "      print(f'Total Comment Extracted: {tot_com} Total Comment Augmented: {len(augmented_rows)} Total Failed Comments: {failed_com}')\n",
        "  df_augmented = pd.DataFrame(augmented_rows)\n",
        "  df_augmented.to_csv(\"df_neu_tail.csv\", index=False)\n",
        "  print(f'Total Comment Extracted: {tot_com} Total Comment Augmented: {len(augmented_rows)} Total Failed Comments: {failed_com}')\n",
        "  print(\"Augmentation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "b8Ukx8Gvx-1Z"
      },
      "outputs": [],
      "source": [
        "df_neu = df.iloc[50000:50600]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jzRdMUy6E4m",
        "outputId": "1d55a95c-6a9c-48b8-e722-99dbc6c7e090"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "600"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "len(df_neu)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "await augmentation_comment_batch(df_neu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "id": "paZ4QoDZNi9Y",
        "outputId": "761ec69a-6a2d-45f1-9da5-f9e99ce83645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total negative comments: 600\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "Error using json.loads on candidate: Invalid \\escape: line 1 column 121 (char 120)\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "Total Comment Extracted: 200 Total Comment Augmented: 200 Total Failed Comments: 0\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "Error using json.loads on candidate: Invalid \\escape: line 1 column 137 (char 136)\n",
            "Error using ast.literal_eval on candidate: unterminated string literal (detected at line 1) (<unknown>, line 1)\n",
            "Error using unicode_escape on candidate: Expecting ',' delimiter: line 1 column 2240 (char 2239)\n",
            "Error using json5.loads on candidate: <string>:1 Unexpected \"D\" at column 1696\n",
            "Error using json.loads on candidate: Invalid \\escape: line 1 column 137 (char 136)\n",
            "Error using ast.literal_eval on candidate: unterminated string literal (detected at line 1) (<unknown>, line 1)\n",
            "Error using unicode_escape on candidate: Expecting ',' delimiter: line 1 column 2240 (char 2239)\n",
            "Error using json5.loads on candidate: <string>:1 Unexpected \"D\" at column 1696\n",
            "Error using json.loads on candidate: Invalid \\escape: line 1 column 137 (char 136)\n",
            "Error using ast.literal_eval on candidate: unterminated string literal (detected at line 1) (<unknown>, line 1)\n",
            "Error using unicode_escape on candidate: Expecting ',' delimiter: line 1 column 2240 (char 2239)\n",
            "Error using json5.loads on candidate: <string>:1 Unexpected \"D\" at column 1696\n",
            "All parsing attempts failed. Final candidate output:\n",
            "[\"I find it incredibly funny that this was released so close to the Oracle vulnerability announcement.\", \"So funny how C++ is\", \"Hi 👋, I\\'m a newbie here.\", \"Regardless of which side suffered more casualties, nobody emerges victorious in this conflict.\", \"Could you clarify which shortcut you used to adjust the font property within the pubspec.yaml file? I didn't catch the specific action when you mentioned zooming back and forth. Could you elaborate?\", \"I\\'m seeking guidance on choosing between the cyber security course offered by Simplilearn and the free course you provide. Can you offer some advice?\", \"That moment when I mistakenly typed Car instead of Cat in the inheritance section -_-\", \"In economics, high costs arise from limited supply and strong demand. In warfare, collateral damage is significant.\", \"Thank you for this insightful video; I've gained a great deal of knowledge today. As a literature graduate expanding my skillset, I'm grateful for this educational program, made possible by a fiction author. He\\'s an emerging African writer revolutionizing African literature with his debut work, Bleeding Stubs by Donald Besong. Your support would encourage him to further sponsor our careers.\", \"The next tutorial should cover website deployment, especially using complimentary platforms like pythonanywhere.\", \"Instead of using a dummy pointer, why not initiate 'left' at the initial node and 'right' at the (first + n)th node, then iterate until 'right.next' equals Null instead of 'right' equaling Null?\", \"That's a significant setback; they might consider initiating a recovery effort, but Turkey is unlikely to permit any Russian warships to transit the strait.\", \"\\\\\"Doesn't want to face scrutiny from a real journalist\\\\\" seriously...\", \"hahaha\", \"Ouch!\", \"@ This highlights the importance of static typing. We need TypeScript integration for ml5, along with a series of TypeScript tutorials for The Coding Train!\", \"What's the process for uploading a 24-hour long video?\", \"hello sir, is pdf available?\", \"For those wondering why the joblib module isn't functioning, keep in mind that joblib isn't included in the latest scikit versions. If you're using Anaconda, simply import joblib directly. Otherwise, run \\\"pip install joblib\\\".\"]\n",
            "robust_json_loads returned None.\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "Error using json.loads on candidate: Expecting value: line 1 column 860 (char 859)\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "Total Comment Extracted: 400 Total Comment Augmented: 380 Total Failed Comments: 20\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "Error using json.loads on candidate: Invalid \\escape: line 1 column 476 (char 475)\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "Error using json.loads on candidate: Invalid \\escape: line 1 column 383 (char 382)\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "len of comments: 20, len of augmented comments: 20\n",
            "Total Comment Extracted: 600 Total Comment Augmented: 580 Total Failed Comments: 20\n",
            "Total Comment Extracted: 600 Total Comment Augmented: 580 Total Failed Comments: 20\n",
            "Augmentation complete!\n"
          ]
        }
      ]
    }
  ]
}